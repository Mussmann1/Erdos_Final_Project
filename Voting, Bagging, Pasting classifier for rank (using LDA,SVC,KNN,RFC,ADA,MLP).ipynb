{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 1 , # of people: 1132\n",
      "rank: 2 , # of people: 1440\n",
      "rank: 3 , # of people: 2470\n",
      "rank: 4 , # of people: 2427\n",
      "rank: 5 , # of people: 2531\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "dfs = pd.read_excel(\"Acme.xlsx\")\n",
    "\n",
    "dfs_copy = dfs.copy()\n",
    "\n",
    "X = dfs_copy[['currently_insured', 'number_of_vehicles', 'number_of_drivers', 'marital_status']]\n",
    "y = dfs_copy['rank']\n",
    "\n",
    "# we take out impression_id since it is just the index + 1.\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(\"rank:\", i,\", # of people:\", dfs_copy.loc[dfs_copy[\"rank\"]==i,\"impression_id\"].count())\n",
    "\n",
    "# thus we have 2531 rank 5 entries\n",
    "# 2427 rank 4 entries\n",
    "# 2470 rank 3 entries\n",
    "# 1440 rank 2 entries\n",
    "# and 1132 rank 1 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    3\n",
       "2    2\n",
       "3    3\n",
       "4    3\n",
       "Name: rank, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['currently_insured','marital_status','number_of_vehicles','number_of_drivers']\n",
    "\n",
    "X[cols] = X[cols].astype(str)\n",
    "\n",
    "#dummies = pd.get_dummies(X[cols])\n",
    "#ydummies = pd.get_dummies(y)\n",
    "\n",
    "# NOTE: here, for multiclass and multi-label classification, we DON'T one-hot encode anything.\n",
    "X_prime = X[cols]\n",
    "\n",
    "# BUT we do have to make string labels into number labels:\n",
    "X_prime.loc[X_prime.currently_insured == \"Y\",'currently_insured'] = 1\n",
    "X_prime.loc[X_prime.currently_insured == \"N\",'currently_insured'] = 0\n",
    "X_prime.loc[X_prime.marital_status == \"M\",'marital_status'] = 1\n",
    "X_prime.loc[X_prime.marital_status == \"S\",'marital_status'] = 0\n",
    "\n",
    "#X_prime = pd.concat([X[[c for c in X.columns if c not in cols]],dummies],axis=1, sort=False)\n",
    "#X_prime = X_prime.astype(int)\n",
    "\n",
    "\n",
    "# This is for predicting on one rank value at a time.\n",
    "    #rank = 1\n",
    "    #y_prime = ydummies.loc[:,rank]\n",
    "\n",
    "    \n",
    "# This is for doing multiple-class rank prediction with ALL ranks 1,2,3,4,5\n",
    "y_prime = y.astype(str)\n",
    "\n",
    "\n",
    "# Here we combine ranks 2,3 into rank 2 and ranks 4,5 into rank 3: \n",
    "y_prime2 = y.astype(str)\n",
    "\n",
    "y_prime2.mask(y_prime2 == \"3\", \"2\", inplace=True)\n",
    "y_prime2.mask(y_prime2 == \"2\", \"2\", inplace=True)\n",
    "y_prime2.mask(y_prime2 == \"1\", \"1\", inplace=True)\n",
    "y_prime2.mask(y_prime2 == \"5\", \"3\", inplace=True)      # we have to do it in this order to avoid conflicts \n",
    "y_prime2.mask(y_prime2 == \"4\", \"3\", inplace=True)      # between the assignments involving \"3\".\n",
    "\n",
    "y_prime2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 1 , # of people: 849\n",
      "rank: 2 , # of people: 1080\n",
      "rank: 3 , # of people: 1853\n",
      "rank: 4 , # of people: 1820\n",
      "rank: 5 , # of people: 1898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_prime,y_prime2,test_size = .25,random_state = 614,shuffle = True,stratify = y)\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(\"rank:\", i, \", # of people:\", X_train.loc[dfs_copy[\"rank\"]==i,\"marital_status\"].count())\n",
    "\n",
    "# So X_train has 849 rank 1 people\n",
    "# and 1080 rank 2 people\n",
    "# and 1853 rank 3 people\n",
    "# and 1820 rank 4 people\n",
    "# and 1898 rank 5 people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    3718\n",
       "3    3718\n",
       "2    3718\n",
       "Name: rank, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is for balancing the training dataset:\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "\n",
    "X_train,y_train = oversample.fit_resample(X_train,y_train)\n",
    "y_train.value_counts()\n",
    "# So y_train, oversampled, has 3718 of rank 1, 2, and 3 people (after making ranks 4,5 --> 3, and 2,3 --> 2.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes 15-20 minutes to run, be careful!\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Here we try a neural network of 5 hidden layers, 65 nodes per layer.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(65,5),max_iter=1000000)\n",
    "\n",
    "svc = SVC(kernel = 'linear', probability = True)                   # linear kernel seems to work best for SVC.\n",
    "rfc = RandomForestClassifier(max_depth = 10,n_estimators = 500)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "bag_knn_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
    "                            n_estimators = 1000,\n",
    "                            max_samples = 1000,\n",
    "                            bootstrap = True)\n",
    "\n",
    "# Here we use a bagging neural network (only 10 estimators), as this was said to improve neural nets during class.\n",
    "bag_mlp_clf = BaggingClassifier(MLPClassifier(hidden_layer_sizes=(65,5),max_iter=1000),\n",
    "                            n_estimators = 10,\n",
    "                            max_samples = 1000,\n",
    "                            bootstrap = True)\n",
    "\n",
    "\n",
    "paste_knn_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
    "                            n_estimators = 1000,\n",
    "                            max_samples = 1000,\n",
    "                            bootstrap = False)\n",
    "\n",
    "# Here we try AdaBoost with RFC. Maybe some other weak learner algorithm will work better.\n",
    "ada_clf = AdaBoostClassifier(RandomForestClassifier(max_depth = 10,n_estimators = 500),\n",
    "                n_estimators = 50,\n",
    "                algorithm=\"SAMME.R\",\n",
    "                learning_rate = 1)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "                [('lda',lda),\n",
    "                ('rfc',rfc),\n",
    "                ('svc',svc),\n",
    "                ('knn',knn),\n",
    "                ('mlp',mlp),\n",
    "                ('ada',ada_clf)],\n",
    "                voting = \"soft\")\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(5, shuffle = True, random_state=614)\n",
    "\n",
    "\n",
    "\n",
    "# Here we balance the data using SMOTE (combine with RandomUnderSampler? We'll see later...)\n",
    "# source: https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n",
    "# ????????\n",
    "X_train,y_train = oversample.fit_resample(X_train,y_train)\n",
    "\n",
    "\n",
    "# Here 10 is the number of classifiers we're using\n",
    "finalacc = np.empty(10)\n",
    "finalprec = np.empty(10)\n",
    "finalrec = np.empty(10)\n",
    "\n",
    "# Here we run over all classifiers and then do cross-validation.\n",
    "k = 0\n",
    "for name,clf in ([\"LDA\",lda],[\"RFC\",rfc],[\"SVC\",svc],[\"KNN\",knn],[\"MLP\",mlp],[\"AdaBoost_clf\",ada_clf],[\"Voting_clf\",voting_clf],[\"Bagging_KNN_clf\",bag_knn_clf],[\"Bagging_MLP_clf\",bag_mlp_clf],[\"Pasting_KNN_clf\",paste_knn_clf]):\n",
    "    \n",
    "    a = np.empty(5)          # 5 for the number of cv-splits.\n",
    "    p = np.empty(5)\n",
    "    r = np.empty(5)\n",
    "    \n",
    "    j = 0\n",
    "    for train_idx, test_idx in cv.split(X_train,y_train):\n",
    "\n",
    "        X_train2 = X_train.iloc[train_idx]\n",
    "        y_train2 = y_train.iloc[train_idx]\n",
    "        X_test2 = X_train.iloc[test_idx]\n",
    "        y_test2 = y_train.iloc[test_idx]\n",
    "        \n",
    "        \n",
    "        clone_clf = clone(clf)\n",
    "        clone_clf.fit(X_train2,y_train2.ravel())\n",
    "        \n",
    "        y_predict = clone_clf.predict(X_test2)\n",
    "        \n",
    "        #y_predict = 1*(y_prob >= cutoff/100)       # Just make y_predict the max class...otherwise we end up\n",
    "                                                    # with some indices being all rank 1,2,3,4, and 5.\n",
    "        # We basically can't use proba_predict because we have multiple classes of y.    \n",
    "\n",
    "        a[j] = 100*metrics.accuracy_score(y_test2, y_predict)\n",
    "        p[j] = 100*metrics.precision_score(y_test2, y_predict, zero_division = 1,average='macro')\n",
    "        r[j] = 100*metrics.recall_score(y_test2, y_predict, zero_division = 1,average='macro')\n",
    "        \n",
    "        # Using macro as the averaging is the same as taking np.mean of the 5 labels' accuracy, precision, and recall.\n",
    "        \n",
    "        j = j + 1\n",
    "        \n",
    "    # The mean over cross-validations of accuracy, precision, and recall\n",
    "    finalacc[k] = np.mean(a)\n",
    "    finalprec[k] = np.mean(p)\n",
    "    finalrec[k] = np.mean(r)\n",
    "    \n",
    "    k = k + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These results are with ranks 2,3 combined into rank 2 and ranks 4,5 combined into rank 3.\n",
      "Note also that we have oversampled to balance out the training set.\n",
      "\n",
      "LDA\n",
      "accuracy: 62.5336584 %\n",
      "precision: 75.3122462 %\n",
      "recall: 62.5335027 %\n",
      "\n",
      "RFC\n",
      "accuracy: 64.6225888 %\n",
      "precision: 63.9121949 %\n",
      "recall: 64.6221846 %\n",
      "\n",
      "SVC\n",
      "accuracy: 63.0625692 %\n",
      "precision: 70.6158377 %\n",
      "recall: 63.0623574 %\n",
      "\n",
      "KNN\n",
      "accuracy: 51.2829333 %\n",
      "precision: 61.333985 %\n",
      "recall: 51.2818685 %\n",
      "\n",
      "MLP\n",
      "accuracy: 64.2191822 %\n",
      "precision: 62.7053163 %\n",
      "recall: 64.2198633 %\n",
      "\n",
      "AdaBoost_clf\n",
      "accuracy: 64.4253678 %\n",
      "precision: 63.2539915 %\n",
      "recall: 64.4240631 %\n",
      "\n",
      "Voting_clf\n",
      "accuracy: 62.9460818 %\n",
      "precision: 63.8529262 %\n",
      "recall: 62.9455564 %\n",
      "\n",
      "Bagging_KNN_clf\n",
      "accuracy: 64.6225888 %\n",
      "precision: 63.9121949 %\n",
      "recall: 64.6221846 %\n",
      "\n",
      "Bagging_MLP_clf\n",
      "accuracy: 63.8515456 %\n",
      "precision: 62.6145658 %\n",
      "recall: 63.8512135 %\n",
      "\n",
      "Pasting_KNN_clf\n",
      "accuracy: 64.6225888 %\n",
      "precision: 63.9121949 %\n",
      "recall: 64.6221846 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"These results are with ranks 2,3 combined into rank 2 and ranks 4,5 combined into rank 3.\")\n",
    "print(\"Note also that we have oversampled to balance out the training set.\")\n",
    "print()\n",
    "\n",
    "k = 0\n",
    "for name in [\"LDA\",\"RFC\",\"SVC\",\"KNN\",\"MLP\",\"AdaBoost_clf\",\"Voting_clf\",\"Bagging_KNN_clf\",\"Bagging_MLP_clf\",\"Pasting_KNN_clf\"]:\n",
    "    print(name)\n",
    "    print(\"accuracy:\",np.round(finalacc[k],7),\"%\")\n",
    "    print(\"precision:\",np.round(finalprec[k],7),\"%\")\n",
    "    print(\"recall:\",np.round(finalrec[k],7),\"%\")\n",
    "    print()\n",
    "    k = k + 1\n",
    "    \n",
    "# why are all performances the same except KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
