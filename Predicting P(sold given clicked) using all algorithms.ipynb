{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 1 , # of people: 581\n",
      "rank: 2 , # of people: 267\n",
      "rank: 3 , # of people: 375\n",
      "rank: 4 , # of people: 100\n",
      "rank: 5 , # of people: 51\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import meshgrid\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "dfs = pd.read_excel(\"Acme.xlsx\")\n",
    "\n",
    "# Here we are predicting for P(buy | click) thus we look at the subset where click is always 1.\n",
    "dfs_click_copy = dfs.loc[dfs['click'] == 1,].copy()\n",
    "\n",
    "X = dfs_click_copy[['currently_insured', 'number_of_vehicles', 'number_of_drivers', 'marital_status','rank']]\n",
    "y = dfs_click_copy['policies sold']\n",
    "# should we include rank in X????????????????\n",
    "\n",
    "\n",
    "# we take out impression_id since it is just the index + 1.\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(\"rank:\", i,\", # of people:\", dfs_click_copy.loc[dfs_click_copy[\"rank\"]==i,\"impression_id\"].count())\n",
    "\n",
    "# thus we have 51 rank 5 entries\n",
    "# 100 rank 4 entries\n",
    "# 375 rank 3 entries\n",
    "# 267 rank 2 entries\n",
    "# and 581 rank 1 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\pandas\\core\\frame.py:2963: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>currently_insured</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>number_of_vehicles</th>\n",
       "      <th>number_of_drivers</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   currently_insured marital_status number_of_vehicles number_of_drivers rank\n",
       "0                  0              1                  1                 1    1\n",
       "8                  0              1                  2                 1    3\n",
       "16                 0              1                  1                 2    2\n",
       "19                 1              1                  2                 1    5\n",
       "27                 0              0                  1                 2    1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['currently_insured','marital_status','number_of_vehicles','number_of_drivers','rank']\n",
    "\n",
    "X[cols] = X[cols].astype(str)\n",
    "\n",
    "#dummies = pd.get_dummies(X[cols])\n",
    "#ydummies = pd.get_dummies(y)\n",
    "\n",
    "# NOTE: here, for multiclass and multi-label classification, we DON'T one-hot encode anything.\n",
    "X_prime = X[cols]\n",
    "\n",
    "# BUT we do have to make purely string labels into number labels:\n",
    "X_prime.loc[X_prime.currently_insured == \"Y\",'currently_insured'] = 1\n",
    "X_prime.loc[X_prime.currently_insured == \"N\",'currently_insured'] = 0\n",
    "X_prime.loc[X_prime.marital_status == \"M\",'marital_status'] = 1\n",
    "X_prime.loc[X_prime.marital_status == \"S\",'marital_status'] = 0\n",
    "\n",
    "#X_prime = pd.concat([X[[c for c in X.columns if c not in cols]],dummies],axis=1, sort=False)\n",
    "#X_prime = X_prime.astype(int)\n",
    "\n",
    "\n",
    "# This is for predicting on one rank value at a time.\n",
    "    #rank = 1\n",
    "    #y_prime = ydummies.loc[:,rank]\n",
    "\n",
    "\n",
    "y_prime = y.astype(str)\n",
    "\n",
    "\n",
    "# Here we combine ranks 2,3 into rank 2 and ranks 4,5 into rank 3: \n",
    "# Don't do this?\n",
    "\n",
    "#X_prime.loc[X_prime['rank'] == \"3\",'rank'] = \"2\"\n",
    "#X_prime.loc[X_prime['rank'] == \"2\",'rank'] = \"2\"\n",
    "#X_prime.loc[X_prime['rank'] == \"1\",'rank'] = \"1\"\n",
    "#X_prime.loc[X_prime['rank'] == \"5\",'rank'] = \"3\"         # we have to do it in this order to avoid conflicts \n",
    "#X_prime.loc[X_prime['rank'] == \"4\",'rank'] = \"3\"         # between the assignments involving \"3\".\n",
    "\n",
    "X_prime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 1 , # of people: 436\n",
      "rank: 2 , # of people: 200\n",
      "rank: 3 , # of people: 281\n",
      "rank: 4 , # of people: 75\n",
      "rank: 5 , # of people: 38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_prime,y_prime,test_size = .25,random_state = 614,shuffle = True,stratify = X_prime[\"rank\"])\n",
    "\n",
    "\n",
    "# We are stratifying by y (= policies sold) but we might want to stratify by rank instead...\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(\"rank:\", i, \", # of people:\", X_train.loc[X_train[\"rank\"]==str(i),\"rank\"].count())\n",
    "\n",
    "# So X_train has 447 rank 1 people\n",
    "# and 473 rank 2 people (original ranks 2 and 3)\n",
    "# and 110 rank 3 people (original ranks 4 and 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 1 , # of people: 528\n",
      "rank: 2 , # of people: 237\n",
      "rank: 3 , # of people: 336\n",
      "rank: 4 , # of people: 88\n",
      "rank: 5 , # of people: 52\n"
     ]
    }
   ],
   "source": [
    "# This is for balancing the training dataset:\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "\n",
    "X_train,y_train = oversample.fit_resample(X_train,y_train)\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(\"rank:\", i, \", # of people:\", X_train.loc[X_train[\"rank\"]==i,\"rank\"].count())\n",
    "\n",
    "# POSSIBLE PROBLEM: Here we are balancing the training set X for POLICIES SOLD, not for RANK!!\n",
    "# How do we fix this???\n",
    "    \n",
    "# So X_train, oversampled, has 528 rank 1 people\n",
    "# and 237 rank 2 people\n",
    "# and 336 rank 3 people\n",
    "# and 88 rank 4 people\n",
    "# and 52 rank 5 people\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\python3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# This takes 15-20 minutes to run, be careful!\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Here we try a neural network of 5 hidden layers, 65 nodes per layer.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(65,5),max_iter=10000000)\n",
    "\n",
    "svc = SVC(kernel = 'linear', probability = True)                   # linear kernel seems to work best for SVC.\n",
    "rfc = RandomForestClassifier(max_depth = 10,n_estimators = 500)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "bag_knn_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
    "                            n_estimators = 1000,\n",
    "                            max_samples = 100,\n",
    "                            bootstrap = True)\n",
    "\n",
    "# Here we use a bagging neural network (only 10 estimators), as this was said to improve neural nets during class.\n",
    "bag_mlp_clf = BaggingClassifier(MLPClassifier(hidden_layer_sizes=(65,5),max_iter=1000),\n",
    "                            n_estimators = 10,\n",
    "                            max_samples = 100,\n",
    "                            bootstrap = True)\n",
    "\n",
    "\n",
    "paste_knn_clf = BaggingClassifier(KNeighborsClassifier(20),\n",
    "                            n_estimators = 1000,\n",
    "                            max_samples = 100,\n",
    "                            bootstrap = False)\n",
    "\n",
    "# Here we try AdaBoost with RFC. Maybe some other weak learner algorithm will work better.\n",
    "ada_clf = AdaBoostClassifier(RandomForestClassifier(max_depth = 10,n_estimators = 500),\n",
    "                n_estimators = 50,\n",
    "                algorithm=\"SAMME.R\",\n",
    "                learning_rate = 1)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "                [('lda',lda),\n",
    "                ('rfc',rfc),\n",
    "                ('svc',svc),\n",
    "                ('knn',knn),\n",
    "                ('mlp',mlp),\n",
    "                ('ada',ada_clf)],\n",
    "                voting = \"soft\")\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(5, shuffle = True, random_state=614)\n",
    "\n",
    "\n",
    "\n",
    "# Here we balance the data using SMOTE (combine with RandomUnderSampler? We'll see later...)\n",
    "# source: https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n",
    "# ????????\n",
    "X_train,y_train = oversample.fit_resample(X_train,y_train)\n",
    "\n",
    "\n",
    "# Here 10 is the number of classifiers we're using\n",
    "finalacc = np.empty(10)\n",
    "finalprec = np.empty(10)\n",
    "finalrec = np.empty(10)\n",
    "\n",
    "# Here we run over all classifiers and then do cross-validation.\n",
    "k = 0\n",
    "for name,clf in ([\"LDA\",lda],[\"RFC\",rfc],[\"SVC\",svc],[\"KNN\",knn],[\"MLP\",mlp],[\"AdaBoost_clf\",ada_clf],[\"Voting_clf\",voting_clf],[\"Bagging_KNN_clf\",bag_knn_clf],[\"Bagging_MLP_clf\",bag_mlp_clf],[\"Pasting_KNN_clf\",paste_knn_clf]):\n",
    "    \n",
    "    a = np.empty(5)          # 5 for the number of cv-splits.\n",
    "    p = np.empty(5)\n",
    "    r = np.empty(5)\n",
    "    \n",
    "    j = 0\n",
    "    for train_idx, test_idx in cv.split(X_train,y_train):\n",
    "\n",
    "        X_train2 = X_train.iloc[train_idx]\n",
    "        y_train2 = y_train.iloc[train_idx]\n",
    "        X_test2 = X_train.iloc[test_idx]\n",
    "        y_test2 = y_train.iloc[test_idx]\n",
    "        \n",
    "        \n",
    "        clone_clf = clone(clf)\n",
    "        clone_clf.fit(X_train2,y_train2.ravel())\n",
    "        \n",
    "        y_predict = clone_clf.predict(X_test2)\n",
    "        \n",
    "        #y_predict = 1*(y_prob >= cutoff/100)       # Just make y_predict the max class...otherwise we end up\n",
    "                                                    # with some indices being all rank 1,2,3,4, and 5.\n",
    "        # We basically can't use proba_predict because we have multiple classes of y.    \n",
    "\n",
    "        a[j] = 100*metrics.accuracy_score(y_test2, y_predict)\n",
    "        p[j] = 100*metrics.precision_score(y_test2, y_predict, zero_division = 1,average='macro')\n",
    "        r[j] = 100*metrics.recall_score(y_test2, y_predict, zero_division = 1,average='macro')\n",
    "        \n",
    "        # Using macro as the averaging is the same as taking np.mean of the 5 labels' accuracy, precision, and recall.\n",
    "        \n",
    "        j = j + 1\n",
    "        \n",
    "    # The mean over cross-validations of accuracy, precision, and recall\n",
    "    finalacc[k] = np.mean(a)\n",
    "    finalprec[k] = np.mean(p)\n",
    "    finalrec[k] = np.mean(r)\n",
    "    \n",
    "    k = k + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ranks are left separate here, and we predict for y = policies sold.\n",
      "Note also that we have oversampled to balance out the training set. But this oversampling occurs for policies sold, not rank!\n",
      "\n",
      "LDA\n",
      "accuracy: 61.8563855 %\n",
      "precision: 62.1367003 %\n",
      "recall: 61.8554839 %\n",
      "\n",
      "RFC\n",
      "accuracy: 59.0464257 %\n",
      "precision: 59.3854341 %\n",
      "recall: 59.0406452 %\n",
      "\n",
      "SVC\n",
      "accuracy: 61.4554217 %\n",
      "precision: 61.8959416 %\n",
      "recall: 61.4548387 %\n",
      "\n",
      "KNN\n",
      "accuracy: 55.7683534 %\n",
      "precision: 55.9746513 %\n",
      "recall: 55.7709677 %\n",
      "\n",
      "MLP\n",
      "accuracy: 59.446747 %\n",
      "precision: 59.9543125 %\n",
      "recall: 59.4432258 %\n",
      "\n",
      "AdaBoost_clf\n",
      "accuracy: 59.6080321 %\n",
      "precision: 59.9828169 %\n",
      "recall: 59.6019355 %\n",
      "\n",
      "Voting_clf\n",
      "accuracy: 59.2125301 %\n",
      "precision: 59.7267633 %\n",
      "recall: 59.2180645 %\n",
      "\n",
      "Bagging_KNN_clf\n",
      "accuracy: 58.0880321 %\n",
      "precision: 58.2544493 %\n",
      "recall: 58.0806452 %\n",
      "\n",
      "Bagging_MLP_clf\n",
      "accuracy: 61.6954217 %\n",
      "precision: 62.3160036 %\n",
      "recall: 61.6922581 %\n",
      "\n",
      "Pasting_KNN_clf\n",
      "accuracy: 58.648996 %\n",
      "precision: 58.7514039 %\n",
      "recall: 58.6425806 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"All ranks are left separate here, and we predict for y = policies sold.\")\n",
    "print(\"Note also that we have oversampled to balance out the training set. But this oversampling occurs for policies sold, not rank!\")\n",
    "print()\n",
    "\n",
    "k = 0\n",
    "for name in [\"LDA\",\"RFC\",\"SVC\",\"KNN\",\"MLP\",\"AdaBoost_clf\",\"Voting_clf\",\"Bagging_KNN_clf\",\"Bagging_MLP_clf\",\"Pasting_KNN_clf\"]:\n",
    "    print(name)\n",
    "    print(\"accuracy:\",np.round(finalacc[k],7),\"%\")\n",
    "    print(\"precision:\",np.round(finalprec[k],7),\"%\")\n",
    "    print(\"recall:\",np.round(finalrec[k],7),\"%\")\n",
    "    print()\n",
    "    k = k + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
